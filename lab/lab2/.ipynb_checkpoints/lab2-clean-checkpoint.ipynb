{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PicturesDataset(Dataset):\n",
    "    \"\"\"Pictures dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on sample.\n",
    "        \"\"\"\n",
    "        self.pic_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def resize(self, image):\n",
    "        # Resize\n",
    "        transformer = transforms.Compose([\n",
    "            transforms.Resize(size=(32, 32)),\n",
    "            transforms.ToTensor()])\n",
    "        image = transformer(image)\n",
    "        return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return: how many pictures are in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.pic_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.pic_frame.iloc[idx, 0])\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.pic_frame.iloc[idx, 0] + '.jpg')\n",
    "        \n",
    "        # image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        tags = self.pic_frame.iloc[idx, 1].split()\n",
    "        tags = [int(x) for x in tags]\n",
    "        tags = multi_hot(tags, 17)\n",
    "        sample = {'image': image, 'tags': tags}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.resize(sample['image'])\n",
    "            \n",
    "        return sample['image'], sample['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_hot(labels, n):\n",
    "    \"\"\"\n",
    "    @labels : a 2D numpy array of indices for multi-hot encoding \n",
    "    @n: number of labels available \n",
    "    Output: assume m labels input. return a m by n tensor. \n",
    "    \"\"\"\n",
    "    size = len(labels)\n",
    "    labels = torch.LongTensor(labels).view(1, -1)  # labels have to be 2D so need the view() function\n",
    "    a = torch.zeros(1, size).long()\n",
    "    i = torch.cat((a, labels))  # indices is a 2D vector..\n",
    "    v = torch.ones(size)\n",
    "    out = torch.sparse.FloatTensor(i, v, torch.Size([1,n])).to_dense()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(out, labels):\n",
    "    \"\"\"\n",
    "    A function to calcualte top1 and top3 precision of predictions.\n",
    "    @out: the output of the final layer of the network. n by 17 tensor, n is the batch size\n",
    "    @labels: the original labels. n by 17 tensor where n is the batch size\n",
    "    \"\"\"\n",
    "    def intersection(lst1, lst2): \n",
    "        lst3 = [value for value in lst1 if value in lst2] \n",
    "        return lst3 \n",
    "\n",
    "    true = labels.nonzero().numpy().tolist()  # all the nonzero values. list of pairs, 0th is the row id and 1th is col id\n",
    "    \n",
    "    rows = np.arange(250).tolist()  # list of row ids\n",
    "    \n",
    "    top1_ids = torch.topk(out, 1, dim=1)[1].squeeze().numpy().tolist()\n",
    "    top1_pred = [[a, b] for (a, b) in zip(rows, top1_ids)]    # top1 predictions\n",
    "    top1_correct = intersection(true, top1_pred)\n",
    "    top1_precision = len(top1_correct)/len(top1_pred)\n",
    "\n",
    "    top3_ids = torch.topk(out, 3, dim=1)[1].squeeze().numpy().tolist()\n",
    "    \n",
    "    top3_pred = []\n",
    "    count = 0\n",
    "    for x in top3_ids:\n",
    "        r = [count] * 3  # row id \n",
    "        pairs = [[a, b] for (a, b) in zip(r, x)]  # make pairs\n",
    "        top3_pred.extend(pairs)  # add to results\n",
    "        count += 1\n",
    "\n",
    "    top3_correct = intersection(true, top3_pred)\n",
    "    top3_precision = len(top3_correct)/len(top3_pred)\n",
    "    \n",
    "    return top1_precision, top3_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_dataset = PicturesDataset(csv_file='kaggleamazon/train.csv',\n",
    "                                   root_dir='kaggleamazon/train-jpg/',\n",
    "                                   transform = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=complete_dataset, \n",
    "                                           batch_size=250, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),            \n",
    "            nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2304, 256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256, 17),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(-1, 2304)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = 17\n",
    "learning_rate = 0.01\n",
    "model = ConvNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/120], Loss: 0.6151, Precision@1: 0.8400, Precision@3: 0.6053\n",
      "Epoch [1/1], Step [20/120], Loss: 0.3729, Precision@1: 0.9200, Precision@3: 0.6267\n",
      "Epoch [1/1], Step [30/120], Loss: 0.2969, Precision@1: 0.9120, Precision@3: 0.6227\n",
      "Epoch [1/1], Step [40/120], Loss: 0.2965, Precision@1: 0.9520, Precision@3: 0.6413\n",
      "Epoch [1/1], Step [50/120], Loss: 0.2701, Precision@1: 0.9120, Precision@3: 0.6227\n",
      "Epoch [1/1], Step [60/120], Loss: 0.2628, Precision@1: 0.9080, Precision@3: 0.6293\n",
      "Epoch [1/1], Step [70/120], Loss: 0.2582, Precision@1: 0.9400, Precision@3: 0.6613\n",
      "Epoch [1/1], Step [80/120], Loss: 0.2544, Precision@1: 0.9200, Precision@3: 0.6333\n",
      "Epoch [1/1], Step [90/120], Loss: 0.2632, Precision@1: 0.9160, Precision@3: 0.6400\n",
      "Epoch [1/1], Step [100/120], Loss: 0.2816, Precision@1: 0.8960, Precision@3: 0.6213\n",
      "Epoch [1/1], Step [110/120], Loss: 0.2559, Precision@1: 0.9320, Precision@3: 0.6467\n",
      "Epoch [1/1], Step [120/120], Loss: 0.2601, Precision@1: 0.9160, Precision@3: 0.6333\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        images = images[:, :3, :, :]  # drop the 4th channel\n",
    "        labels = labels.squeeze()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        precision1, precision3 = precision(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Precision@1: {:.4f}, Precision@3: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(), precision1, precision3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
