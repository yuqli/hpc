{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5b604461a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#!/anaconda/envs/nlp/bin/python\n",
    "\n",
    "### 20181027\n",
    "### Yuqiong Li\n",
    "### HPC Lab2\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image  # read images\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#                                   Data setup\n",
    "############################################################################################\n",
    "class PicturesDataset(Dataset):\n",
    "    \"\"\"Pictures dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on sample.\n",
    "        \"\"\"\n",
    "        self.pic_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def resize(self, image):\n",
    "        # Resize\n",
    "        transformer = transforms.Compose([\n",
    "            transforms.Resize(size=(32, 32)),\n",
    "            transforms.ToTensor()])\n",
    "        image = transformer(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return: how many pictures are in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.pic_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.pic_frame.iloc[idx, 0])\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.pic_frame.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "        # image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        tags = self.pic_frame.iloc[idx, 1].split()\n",
    "        tags = [int(x) for x in tags]\n",
    "        tags = multi_hot(tags, self.num_classes)\n",
    "        sample = {'image': image, 'tags': tags}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.resize(sample['image'])\n",
    "\n",
    "        return sample['image'], sample['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#                                   Network setup\n",
    "############################################################################################\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2304, 256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(-1, 2304)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def multi_hot(labels, n):\n",
    "    \"\"\"\n",
    "    @labels : a 2D numpy array of indices for multi-hot encoding\n",
    "    @n: number of labels available\n",
    "    Output: assume m labels input. return a m by n tensor.\n",
    "    \"\"\"\n",
    "    size = len(labels)\n",
    "    labels = torch.LongTensor(labels).view(1, -1)  # labels have to be 2D so need the view() function\n",
    "    a = torch.zeros(1, size).long()\n",
    "    i = torch.cat((a, labels))  # indices is a 2D vector..\n",
    "    v = torch.ones(size)\n",
    "    out = torch.sparse.FloatTensor(i, v, torch.Size([1,n])).to_dense()\n",
    "    return out\n",
    "\n",
    "\n",
    "def precision(out, labels):\n",
    "    \"\"\"\n",
    "    A function to calcualte top1 and top3 precision of predictions.\n",
    "    @out: the output of the final layer of the network. n by 17 tensor, n is the batch size\n",
    "    @labels: the original labels. n by 17 tensor where n is the batch size\n",
    "    \"\"\"\n",
    "\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "\n",
    "    true = labels.nonzero().numpy().tolist()  # all the nonzero values. list of pairs, 0th is the row id and 1th is col id\n",
    "\n",
    "    rows = np.arange(250).tolist()  # list of row ids\n",
    "\n",
    "    top1_ids = torch.topk(out, 1, dim=1)[1].squeeze().numpy().tolist()\n",
    "    top1_pred = [[a, b] for (a, b) in zip(rows, top1_ids)]  # top1 predictions\n",
    "    top1_correct = intersection(true, top1_pred)\n",
    "    top1_precision = len(top1_correct) / len(top1_pred)\n",
    "\n",
    "    top3_ids = torch.topk(out, 3, dim=1)[1].squeeze().numpy().tolist()\n",
    "\n",
    "    top3_pred = []\n",
    "    count = 0\n",
    "    for x in top3_ids:\n",
    "        r = [count] * 3  # row id\n",
    "        pairs = [[a, b] for (a, b) in zip(r, x)]  # make pairs\n",
    "        top3_pred.extend(pairs)  # add to results\n",
    "        count += 1\n",
    "\n",
    "    top3_correct = intersection(true, top3_pred)\n",
    "    top3_precision = len(top3_correct) / len(top3_pred)\n",
    "\n",
    "    return top1_precision, top3_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#                                   Training setup\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "\n",
    "    # parse arguments\n",
    "    parser = argparse.ArgumentParser(add_help=False, description=('Begin running your computer vision experiments.'))\n",
    "    parser.add_argument('--help', '-h', action='help', default=argparse.SUPPRESS, help='Show this help message and exit')\n",
    "    parser.add_argument('--device', '-d', help='Speicfy the type of device to use')\n",
    "    parser.add_argument('--path', '-p', help='Path of data folder')\n",
    "    parser.add_argument('--workers', '-w', type=int, help='Number of dataloader workers')\n",
    "    parser.add_argument('--optimizer', '-o', help='Type of optimizer to use')\n",
    "\n",
    "\n",
    "    try:\n",
    "        args = parser.parse_args(argv)\n",
    "\n",
    "        device_specify = args.device\n",
    "        path = args.path\n",
    "        num_workers = args.workers\n",
    "        optimizer = args.optimizer\n",
    "\n",
    "        if not path:\n",
    "            parser.print_usage()\n",
    "            raise ValueError('you need to specify an input path for pictures')\n",
    "\n",
    "        if not device_specify:\n",
    "            parser.print_usage()\n",
    "            raise ValueError('you need to specify a device to use for training')\n",
    "\n",
    "        if not num_workers:\n",
    "            parser.print_usage()\n",
    "            raise ValueError('how many workers do you want to laod data for you?')\n",
    "\n",
    "        if not optimizer:\n",
    "            parser.print_usage()\n",
    "            raise ValueError('you need to speicify the type of optimizer you want to use')\n",
    "\n",
    "\n",
    "        # hardware hyperparameters\n",
    "        device = torch.device('cuda' if device_specify == 'gpu' else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "optimizer = 'sdg'\n",
    "num_workers = 24\n",
    "path = '/scratch/am9031/CSCI-GA.3033-022/lab2/kaggleamazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "num_classes = 17\n",
    "\n",
    "# training hyperparameters\n",
    "num_epochs = 5 \n",
    "\n",
    "csv_file_path = os.path.join(path, 'train.csv')\n",
    "root_dir_path = os.path.join(path, 'train-jpg')\n",
    "\n",
    "# load data\n",
    "complete_dataset = PicturesDataset(csv_file=csv_file_path,\n",
    "                                   root_dir=root_dir_path,\n",
    "                                   transform=True,\n",
    "                                   num_classes=num_classes)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=complete_dataset,\n",
    "                                           batch_size=250,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "if(optimizer==\"sgd\"):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif (optimizer==\"sgd-nesterov\"):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "elif (optimizer==\"adagrad\"):\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "elif (optimizer==\"adadelta\"):\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "elif (optimizer==\"adam\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_time = 0\n",
    "    waiting_time = 0\n",
    "    compute_time = 0\n",
    "\n",
    "    start = time.monotonic()\n",
    "    flag = time.monotonic()  # reset flag\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        waiting_time += time.monotonic() - flag # aggregate on waiting time\n",
    "        flag = time.monotonic()  # reset flag\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = images[:, :3, :, :]  # drop the 4th channel\n",
    "        labels = labels.squeeze()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        precision1, precision3 = precision(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        compute_time += time.monotonic() - flag  # aggregate compute time\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Precision@1: {:.4f}, Precision@3: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), precision1, precision3))\n",
    "        flag = time.monotonic()\n",
    "\n",
    "    print('\\nDone!')\n",
    "    epoch_time += time.monotonic() - start\n",
    "    # round time\n",
    "   # epoch_time = round((epoch_time) * 10 ** (-9), 10)\n",
    "   # waiting_time = round((waiting_time) * 10 ** (-9), 10)\n",
    "   # compute_time = round((compute_time) * 10 ** (-9), 10)\n",
    "    fname = \"result{}{}.txt\".format(optimizer, num_workers)\n",
    "    f = open(fname, 'a')\n",
    "    f.write(\"Optimizer {}\\n\".format(optimizer))\n",
    "    f.write(\"Number of workers {}\\n\".format(num_workers))\n",
    "    f.write(\"This is epoch\" + str(epoch)+'\\n')\n",
    "    f.write(\"Waiting time : \" + str(waiting_time) + \" secs\\n\")\n",
    "    f.write(\"Compute time : \" + str(compute_time) + \" secs\\n\")\n",
    "    f.write(\"Epoch time : \" + str(epoch_time) + \" secs\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
